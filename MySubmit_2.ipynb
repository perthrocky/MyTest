{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from mlxtend.data import boston_housing_data\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pal = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------merge ans-------\n",
    "BASE_DIR = '../../data/'\n",
    "\n",
    "df_test = pd.read_csv(BASE_DIR+'Quora/input/test.csv')\n",
    "\n",
    "lystdo_1_ans = pd.read_csv(BASE_DIR+'Quora/input/lystdo_ans_lstm_193_108_0.33_0.22_1.csv')\n",
    "lystdo_ft_ans = pd.read_csv(BASE_DIR+'Quora/input/lystdo_ans_lstm_175_117_0.20_0.15_ft.csv')\n",
    "hadry_ans = pd.read_csv(BASE_DIR+'Quora/input/hadry_ans_6.csv')\n",
    "\n",
    "myans = pd.DataFrame()\n",
    "myans['test_id'] = df_test['test_id']\n",
    "myans['is_duplicate'] = lystdo_1_ans['is_duplicate']*0.1+lystdo_ft_ans['is_duplicate']*0.1+ hadry_ans['is_duplicate']*0.8\n",
    "\n",
    "myans.to_csv(BASE_DIR+'Quora/input/my_ans_21.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------load ori data--------\n",
    "df_train = pd.read_csv('../../data/Quora/input/train.csv')\n",
    "df_test = pd.read_csv('../../data/Quora/input/test.csv')\n",
    "df_train = df_train.fillna(' ')\n",
    "df_test = df_test.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------load hadry feature file--------\n",
    "hadry_x_train = pd.read_csv('../../data/Quora/feature/hadry_X_train.csv', header=0)\n",
    "hadry_x_test  = pd.read_csv('../../data/Quora/feature/hadry_X_test.csv', header=0)\n",
    "\n",
    "hadry_x_train=hadry_x_train.rename(columns = {'q1_q2_intersect.1':'q1_q2_intersect'})\n",
    "hadry_x_train=hadry_x_train.rename(columns = {'q1_freq.1':'q1_freq'})\n",
    "hadry_x_train=hadry_x_train.rename(columns = {'q2_freq.1':'q2_freq'})\n",
    "\n",
    "hadry_x_test=hadry_x_test.rename(columns = {'q1_q2_intersect.1':'q1_q2_intersect'})\n",
    "hadry_x_test=hadry_x_test.rename(columns = {'q1_freq.1':'q1_freq'})\n",
    "hadry_x_test=hadry_x_test.rename(columns = {'q2_freq.1':'q2_freq'})\n",
    "\n",
    "#--------Add ad feature------\n",
    "abh_x_train = pd.read_csv('../../data/Quora/feature/Abhishek/train_features.csv', header=0)\n",
    "abh_x_test  = pd.read_csv('../../data/Quora/feature/Abhishek/test_features.csv', header=0)\n",
    "\n",
    "abh_x_train = abh_x_train.drop('euclidean_distance', axis=1)\n",
    "abh_x_train = abh_x_train.drop('jaccard_distance', axis=1)\n",
    "abh_x_train = abh_x_train.drop('question1', axis=1)\n",
    "abh_x_train = abh_x_train.drop('question2', axis=1)\n",
    "\n",
    "abh_x_test = abh_x_test.drop('euclidean_distance', axis=1)\n",
    "abh_x_test = abh_x_test.drop('jaccard_distance', axis=1)\n",
    "abh_x_test = abh_x_test.drop('question1', axis=1)\n",
    "abh_x_test = abh_x_test.drop('question2', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------merge feature------\n",
    "x_train = pd.concat((hadry_x_train, abh_x_train), axis=1)\n",
    "x_test  = pd.concat((hadry_x_test,  abh_x_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "404290\n",
      "2345796\n"
     ]
    }
   ],
   "source": [
    "print len(x_train.columns)\n",
    "print len(x_train)\n",
    "print len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'word_match', u'jaccard', u'wc_diff', u'wc_ratio', u'wc_diff_unique',\n",
       "       u'wc_ratio_unique', u'wc_diff_unq_stop', u'wc_ratio_unique_stop',\n",
       "       u'same_start', u'char_diff', u'char_diff_unq_stop',\n",
       "       u'total_unique_words', u'total_unq_words_stop', u'char_ratio',\n",
       "       u'q1_q2_intersect', u'q1_freq', u'q2_freq', u'len_q1', u'len_q2',\n",
       "       u'diff_len', u'len_char_q1', u'len_char_q2', u'len_word_q1',\n",
       "       u'len_word_q2', u'common_words', u'fuzz_qratio', u'fuzz_WRatio',\n",
       "       u'fuzz_partial_ratio', u'fuzz_partial_token_set_ratio',\n",
       "       u'fuzz_partial_token_sort_ratio', u'fuzz_token_set_ratio',\n",
       "       u'fuzz_token_sort_ratio', u'wmd', u'norm_wmd', u'cosine_distance',\n",
       "       u'cityblock_distance', u'canberra_distance', u'minkowski_distance',\n",
       "       u'braycurtis_distance', u'skew_q1vec', u'skew_q2vec', u'kur_q1vec',\n",
       "       u'kur_q2vec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------Add extra feature: fasttx------\n",
    "ft_x_train = pd.read_csv('../../data/Quora/feature/ft_trainX.csv', header=0)\n",
    "ft_x_test = pd.read_csv('../../data/Quora/feature/ft_testX.csv', header=0)\n",
    "\n",
    "x_train['ft_cosine']     = ft_x_train['ft_cosine']\n",
    "x_train['ft_cityblock']  = ft_x_train['ft_cityblock']\n",
    "x_train['ft_canberra']   = ft_x_train['ft_canberra']\n",
    "x_train['ft_minkowski']  = ft_x_train['ft_minkowski']\n",
    "x_train['ft_braycurtis'] = ft_x_train['ft_braycurtis']\n",
    "\n",
    "x_test['ft_cosine']     = ft_x_test['ft_cosine']\n",
    "x_test['ft_cityblock']  = ft_x_test['ft_cityblock']\n",
    "x_test['ft_canberra']   = ft_x_test['ft_canberra']\n",
    "x_test['ft_minkowski']  = ft_x_test['ft_minkowski']\n",
    "x_test['ft_braycurtis'] = ft_x_test['ft_braycurtis']\n",
    "\n",
    "gv_x_train = pd.read_csv('../../data/Quora/feature/gv_trainX.csv', header=0)\n",
    "gv_x_test = pd.read_csv('../../data/Quora/feature/gv_testX.csv', header=0)\n",
    "\n",
    "x_train['gv_cosine']     = gv_x_train['gv_cosine']\n",
    "x_train['gv_cityblock']  = gv_x_train['gv_cityblock']\n",
    "x_train['gv_canberra']   = gv_x_train['gv_canberra']\n",
    "x_train['gv_minkowski']  = gv_x_train['gv_minkowski']\n",
    "x_train['gv_braycurtis'] = gv_x_train['gv_braycurtis']\n",
    "\n",
    "x_test['gv_cosine']     = gv_x_test['gv_cosine']\n",
    "x_test['gv_cityblock']  = gv_x_test['gv_cityblock']\n",
    "x_test['gv_canberra']   = gv_x_test['gv_canberra']\n",
    "x_test['gv_minkowski']  = gv_x_test['gv_minkowski']\n",
    "x_test['gv_braycurtis'] = gv_x_test['gv_braycurtis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n",
      "404290\n",
      "404290\n",
      "2345796\n"
     ]
    }
   ],
   "source": [
    "print len(x_train.columns)\n",
    "print len(x_test.columns)\n",
    "print len(x_train)\n",
    "print len(y_train)\n",
    "print len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189752932122\n",
      "0.189234677675\n",
      "[0]\ttrain-logloss:0.476034\tvalid-logloss:0.475378\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.281815\tvalid-logloss:0.282516\n",
      "[100]\ttrain-logloss:0.234631\tvalid-logloss:0.235906\n",
      "[150]\ttrain-logloss:0.217952\tvalid-logloss:0.219596\n",
      "[200]\ttrain-logloss:0.210505\tvalid-logloss:0.212342\n",
      "[250]\ttrain-logloss:0.205289\tvalid-logloss:0.207451\n",
      "[300]\ttrain-logloss:0.201175\tvalid-logloss:0.203609\n",
      "[350]\ttrain-logloss:0.198209\tvalid-logloss:0.200893\n",
      "[400]\ttrain-logloss:0.19588\tvalid-logloss:0.198784\n",
      "[450]\ttrain-logloss:0.193898\tvalid-logloss:0.19712\n",
      "[500]\ttrain-logloss:0.192267\tvalid-logloss:0.195765\n",
      "[550]\ttrain-logloss:0.190804\tvalid-logloss:0.194602\n",
      "[600]\ttrain-logloss:0.189722\tvalid-logloss:0.193794\n",
      "[650]\ttrain-logloss:0.188643\tvalid-logloss:0.193023\n",
      "[700]\ttrain-logloss:0.187638\tvalid-logloss:0.192387\n",
      "[750]\ttrain-logloss:0.186828\tvalid-logloss:0.191885\n",
      "[800]\ttrain-logloss:0.185961\tvalid-logloss:0.191338\n",
      "[850]\ttrain-logloss:0.18517\tvalid-logloss:0.190913\n",
      "[900]\ttrain-logloss:0.184435\tvalid-logloss:0.190539\n",
      "[950]\ttrain-logloss:0.183723\tvalid-logloss:0.19014\n",
      "[999]\ttrain-logloss:0.18307\tvalid-logloss:0.189819\n",
      "0.189819312041\n"
     ]
    }
   ],
   "source": [
    "#------train xgb\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=4242)\n",
    "\n",
    "#UPDownSampling\n",
    "pos_train = X_train[y_train == 1]\n",
    "neg_train = X_train[y_train == 0]\n",
    "X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8*len(pos_train))], neg_train))\n",
    "y_train = np.array([0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8*len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "print(np.mean(y_train))\n",
    "del pos_train, neg_train\n",
    "\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "y_valid = np.array([0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "print(np.mean(y_valid))\n",
    "del pos_valid, neg_valid\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 5\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    "# params['scale_pos_weight'] = 0.2\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "\n",
    "#------TEST xgb\n",
    "\n",
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('../../data/Quora/input/my_ans_16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
